**ğŸš€ PySpark Learning Journey â€” Manikandan**

Welcome to my PySpark Learning Journey repository!
This repo documents my hands-on practice, daily exercises, use-cases, and end-to-end data engineering scenarios using PySpark, SQL, and big-data concepts.

My goal is to build strong, interview-ready skills in PySpark and Data Engineering.

**ğŸ“˜ What This Repository Contains**

âœ… 1. **Daily PySpark Coding Practice**

Transformations & Actions

Joins, Aggregations, Window functions

Date & Time functions

Handling nulls, duplicates, and data quality rules

UDFs, performance tuning basics

âœ… 2. **Scenario-Based Real DE Problems**

Every folder includes real interview-style questions such as:

Detecting missing dates

Identifying duplicates

SCD & CDC handling

Data quality validation

Skew handling

Partitioning & performance tuning

âœ… 3. **End-to-End Mini Projects**

Practical PySpark pipelines:

Reading from CSV/JSON/Parquet

Applying business rules

Writing optimized output

Partitioning, bucketing, and caching

Sample Airflow DAGs for orchestration

âœ… 4. **SQL + PySpark Equivalent Solutions**

Side-by-side comparison:

SQL queries

PySpark DataFrame solutions

Window functions

Optimization tips

**ğŸ“‚ Repository Structure**

â”œâ”€â”€ data/

â”œâ”€â”€ scenario/

â”œâ”€â”€ src/

â””â”€â”€ README.md

*ğŸ¯ Learning Goals*

Write clean, optimized PySpark code

Think like a Data Engineer, not just a coder

Master window functions

Build pipeline-ready transformations

Improve debugging + logging skills

Prepare for real DE interviews

*ğŸ§  Topics I am Mastering*

Spark Architecture (Driver, Executors)

Lazy evaluation

Shuffle, Skew, Partitions

Caching & broadcast joins

Narrow vs wide transformations

ETL pipeline design

*ğŸ“ How I Practice*

Pick one real scenario every day

Solve it using PySpark

Push code + explanation

Track mistakes & improvements
